{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd633618",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e68872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c3360",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7535f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_params():\n",
    "    with open(\"../params.yaml\") as f:\n",
    "        params = yaml.safe_load(f)\n",
    "    return params\n",
    "\n",
    "# Load params\n",
    "params = load_params()\n",
    "\n",
    "# Load recipes data\n",
    "data = pd.read_pickle(\"../\" + params[\"model_pipeline\"][\"recipe_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9c9f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': 'models/fasttext.model',\n",
       " 'model_ngram_path': 'models/fasttext.model.wv.vectors_ngrams.npy',\n",
       " 'recipe_path': 'data/processed_cookbook.pkl',\n",
       " 'fast_text': {'vector_size': 100,\n",
       "  'window': 8,\n",
       "  'sg': 0,\n",
       "  'epochs': 100,\n",
       "  'hs': 1,\n",
       "  'seed': 1234,\n",
       "  'min_count': 1},\n",
       " 'model_scoring': {'w_title': 0.2,\n",
       "  'w_cosine': 0.2,\n",
       "  'w_maxsim': 0.6,\n",
       "  'idf_top_weights': [0.8, 0.1, 0.1]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load current model metadata\n",
    "with open(\"../training_metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata[\"model_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549788ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"FastText-Model\")\n",
    "\n",
    "# Load mlflow model\n",
    "model = mlflow.pyfunc.load_model(\"../\" + metadata[\"local_uri\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586806b",
   "metadata": {},
   "source": [
    "## Ground Truth Similarity Scores\n",
    "\n",
    "Based only on ingredients as we want to make sure it includes all the ingredients in the query and not too much other ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "30f1fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_ings  = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), norm=\"l2\")\n",
    "X_ings  = tfidf_ings.fit_transform(data.ingredients.apply(lambda x: \", \".join(x)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "f2b7788b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03605603, 0.06025482, 0.09246626, ..., 0.00964642, 0.08279631,\n",
       "       0.00443774])"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "q_id = random.randint(0, len(data.ingredients))\n",
    "q_ings = X_ings[q_id]\n",
    "true_scores = cosine_similarity(q_ings, X_ings)[0]\n",
    "true_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb92768",
   "metadata": {},
   "source": [
    "## Model Prediction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "732446e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39872128, 0.36949491, 0.39284569, ..., 0.18476525, 0.45841201,\n",
       "       0.06926141])"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores = model.predict(data.ingredients.iloc[q_id])\n",
    "pred_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498a9c9",
   "metadata": {},
   "source": [
    "## Predictive Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70d9ed",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "b9f522ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2612618074672088\n",
      "MSE: 0.06825773204103287\n",
      "MAE: 0.23503921615713305\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean((true_scores - pred_scores)**2))\n",
    "mse = np.mean((true_scores - pred_scores)**2)\n",
    "mae = np.mean(np.abs(true_scores - pred_scores))\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8598c1",
   "metadata": {},
   "source": [
    "### Soft Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "3c524dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.14232544, 0.0710299 , 0.126302  , 0.2394034 ,\n",
       "       0.12145707, 0.06825931, 0.08254903, 0.06712469, 0.06515801])"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=10\n",
    "recommended_idx = np.argsort(pred_scores)[::-1][:30]\n",
    "true_preds = true_scores[recommended_idx]\n",
    "true_preds[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "3c72d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K: 0.19836088555085438\n",
      "Recall@K: 0.56162399110598\n",
      "F1@K: 0.29317486609063437\n"
     ]
    }
   ],
   "source": [
    "precision_k = true_preds[:k].mean() # Precision@K\n",
    "recall_k = true_preds[:k].sum()/true_preds.sum() # Recall@K\n",
    "f1_k = (2 * precision_k * recall_k) / (precision_k + recall_k) # F1@K\n",
    "\n",
    "print(\"Precision@K:\", precision_k)\n",
    "print(\"Recall@K:\", recall_k)\n",
    "print(\"F1@K:\", f1_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb91d88",
   "metadata": {},
   "source": [
    "## Ranking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "03b79b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03605603, 0.06025482, 0.09246626, ..., 0.00964642, 0.08279631,\n",
       "       0.00443774])"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "q_id = random.randint(0, len(data.ingredients))\n",
    "q_ings = X_ings[q_id]\n",
    "true_scores = cosine_similarity(q_ings, X_ings)[0]\n",
    "true_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfc7b9",
   "metadata": {},
   "source": [
    "### NDCG (Normalized Discounted Cumulative Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "2ac3cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.8098711508388237\n"
     ]
    }
   ],
   "source": [
    "sorted_recs = sorted(true_scores, reverse=True)\n",
    "dcg, idcg = 0, 0\n",
    "for i in range(len(true_scores)):\n",
    "    dcg += true_scores[i]/np.log2((i+1)+1)\n",
    "    idcg += sorted_recs[i]/np.log2((i+1)+1)\n",
    "\n",
    "idcg += 1e-10  # Avoid dividing by 0\n",
    "ndcg = dcg/idcg\n",
    "print(f\"NDCG: {ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb8af1",
   "metadata": {},
   "source": [
    "### AP (Average Precision) --> MAP (Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "68dd5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: nan\n"
     ]
    }
   ],
   "source": [
    "relevant_preds = true_preds[1:] > 0.5\n",
    "relevant_count = np.cumsum(relevant_preds)[relevant_preds == 1]\n",
    "relevant_idx = np.arange(1, len(relevant_preds) + 1)[relevant_preds == 1]\n",
    "ap = np.nanmean((relevant_count / relevant_idx))\n",
    "print(f\"AP: {ap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7c53f",
   "metadata": {},
   "source": [
    "### RR (Reciprocal Rank) --> MRR (Mean Reciprocal Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "eda410b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0\n"
     ]
    }
   ],
   "source": [
    "first_relevant_idx = relevant_idx[0] if len(relevant_idx) > 0 else 0\n",
    "rr = 1/first_relevant_idx if first_relevant_idx else 0\n",
    "print(f\"RR: {rr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e92683",
   "metadata": {},
   "source": [
    "## Repeat to ensure consistency\n",
    "\n",
    "Source: https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems#mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "51456e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           RMSE: 0.22691767167952398\n",
      "            MSE: 0.053161452762294795\n",
      "            MAE: 0.1940248615680723\n",
      "   Precision_10: 0.3394571886911364\n",
      "      Recall_10: 0.44860077437530793\n",
      "          F1_10: 0.3664164538518096\n",
      "           NDCG: 0.7961161608436464\n",
      "            MAP: 0.19081783956783954\n",
      "            MRR: 0.2166666666666667\n",
      "       Hit-Rate: 0.3\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "k = 10\n",
    "relevant_thresh = 0.5\n",
    "scores = {\"RMSE\": [], \"MSE\": [], \"MAE\": [], f\"Precision_{k}\": [], f\"Recall_{k}\": [], f\"F1_{k}\": [], \"NDCG\": [], \"MAP\": [], \"MRR\": [], \"Hit-Rate\": []}\n",
    "\n",
    "for i in range(10):\n",
    "    # Get true scores for current query\n",
    "    q_id = random.randint(0, len(data.ingredients))\n",
    "    q_ings = X_ings[q_id]\n",
    "    true_scores = cosine_similarity(q_ings, X_ings)[0]\n",
    "\n",
    "    # Get predicted scores for current query\n",
    "    pred_scores = model.predict(data.ingredients.iloc[q_id])\n",
    "\n",
    "    # Get top recommendations and its true similarity\n",
    "    recommended_idx = np.argsort(pred_scores)[::-1][:params[\"model_service\"][\"n_recs\"]]\n",
    "    true_preds = true_scores[recommended_idx]\n",
    "\n",
    "    # Calculate predictive metrics\n",
    "    ## RMSE\n",
    "    rmse = np.sqrt(np.mean((true_scores - pred_scores)**2))\n",
    "    scores[\"RMSE\"].append(rmse)\n",
    "\n",
    "    ## MSE\n",
    "    mse = np.mean((true_scores - pred_scores)**2)\n",
    "    scores[\"MSE\"].append(mse)\n",
    "\n",
    "    ## MAE\n",
    "    mae = np.mean(np.abs(true_scores - pred_scores))\n",
    "    scores[\"MAE\"].append(mae)\n",
    "\n",
    "    ## Precision@K\n",
    "    precision_k = true_preds[:k].mean()\n",
    "    scores[f\"Precision_{k}\"].append(precision_k)\n",
    "\n",
    "    ## Recall@K\n",
    "    recall_k = true_preds[:k].sum()/true_preds.sum() \n",
    "    scores[f\"Recall_{k}\"].append(recall_k)\n",
    "\n",
    "    ## F1@K\n",
    "    f1_k = (2 * precision_k * recall_k) / (precision_k + recall_k) \n",
    "    scores[f\"F1_{k}\"].append(f1_k)\n",
    "\n",
    "    # Calculate ranking metrics\n",
    "    ## NDCG\n",
    "    sorted_recs = sorted(true_scores, reverse=True)\n",
    "    dcg, idcg = 0, 0\n",
    "    for i in range(len(true_scores)):\n",
    "        dcg += true_scores[i]/np.log2((i+1)+1)\n",
    "        idcg += sorted_recs[i]/np.log2((i+1)+1)\n",
    "\n",
    "    idcg += 1e-10  # Avoid dividing by 0\n",
    "    scores[\"NDCG\"].append(dcg/idcg)\n",
    "\n",
    "    ## MAP\n",
    "    relevant_preds = true_preds[1:] > relevant_thresh\n",
    "    relevant_count = np.cumsum(relevant_preds)[relevant_preds == 1]\n",
    "    relevant_idx = np.arange(1, len(relevant_preds) + 1)[relevant_preds == 1]\n",
    "    ap = np.nanmean((relevant_count / relevant_idx))\n",
    "    scores[\"MAP\"].append(ap if not np.isnan(ap) else 0)\n",
    "\n",
    "    ## MRR\n",
    "    first_relevant_idx = relevant_idx[0] if len(relevant_idx) > 0 else 0\n",
    "    rr = 1/first_relevant_idx if first_relevant_idx else 0\n",
    "    scores[\"MRR\"].append(rr)\n",
    "\n",
    "    ## Hit-Rate\n",
    "    hit = 1 if True in relevant_preds[:k] else 0\n",
    "    scores[\"Hit-Rate\"].append(hit)\n",
    "\n",
    "for metric in scores:\n",
    "    print(f\"{metric:>15}:\", np.mean(scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90577e16",
   "metadata": {},
   "source": [
    "## Update Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "9761482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = {}\n",
    "for metric in scores:\n",
    "    metric_scores[metric] = np.mean(scores[metric])\n",
    "\n",
    "validation_metadata = {**metadata, \"metrics\": metric_scores}\n",
    "\n",
    "# Save validation model metadata\n",
    "with open(\"../validation_metadata.json\", \"w\") as f:\n",
    "    f.write(json.dumps(validation_metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51237cf3",
   "metadata": {},
   "source": [
    "## Log to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "0df96676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run serious-stoat-300 at: http://127.0.0.1:8080/#/experiments/300877920446874715/runs/32cc39d3e1e24799886476ad91037e36\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/300877920446874715\n"
     ]
    }
   ],
   "source": [
    "# Log metrics to MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"FastText-Model\")\n",
    "\n",
    "with mlflow.start_run(run_id=metadata[\"run_id\"]):\n",
    "    mlflow.log_metrics(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1bfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
